{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "39Tx5-YKIqq4",
        "sVJuLg0aK1Xr",
        "r42A3SSNLgEE",
        "qZd1VNunMBhP",
        "SKt4QSppMHIo",
        "WFNLlq4-MXJN"
      ],
      "authorship_tag": "ABX9TyNGXBw60adS3tUE9k7e01rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bashdragon/llm-discussion/blob/main/foundations/learn_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization for Transformers\n",
        "### Lets look at tokenization - the foundation of NLP.\n",
        "-------------------------------------------------"
      ],
      "metadata": {
        "id": "FlbqeahXGkA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Install dependencies and setup the library"
      ],
      "metadata": {
        "id": "39Tx5-YKIqq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers nltk sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REu-5VTrGpRf",
        "outputId": "39f37960-d42e-47b4-98cb-ade531d199a5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4z5W5yDG8aT",
        "outputId": "8b4bd75e-d57a-4ca4-9abe-7b57bdb7619f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhh83hywLTss"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Understanding Tokenization"
      ],
      "metadata": {
        "id": "sVJuLg0aK1Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of splitting text into smaller units, called tokens. There are different types of tokenization:\n",
        "1. Word Tokenization: Splitting text into words.\n",
        "2. Subword Tokenization: Breaking words into meaningful subunits.\n",
        "3. Character Tokenization: Treating each character as a token."
      ],
      "metadata": {
        "id": "cdQi2TBPLSTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Tokenization using NLTK"
      ],
      "metadata": {
        "id": "r42A3SSNLgEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is the foundation of NLP. It helps break text into units. Do you agree?\"\n"
      ],
      "metadata": {
        "id": "o6_wicL9Lnkt"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence level tokenization:"
      ],
      "metadata": {
        "id": "qZd1VNunMBhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbn2B1NqLq8p",
        "outputId": "ea4a1393-4c04-48a2-dd92-66c8a7e9424d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Tokenization is the foundation of NLP.', 'It helps break text into units.', 'Do you agree?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word level tokenization:"
      ],
      "metadata": {
        "id": "SKt4QSppMHIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbUbxe-VMJ-s",
        "outputId": "94524f5d-0146-493c-e131-fb411c40530d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Tokenization', 'is', 'the', 'foundation', 'of', 'NLP', '.', 'It', 'helps', 'break', 'text', 'into', 'units', '.', 'Do', 'you', 'agree', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Sub-word level tokenization:"
      ],
      "metadata": {
        "id": "WFNLlq4-MXJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer models use subword tokenization to handle unknown words efficiently.\n",
        "This prevents out-of-vocabulary (OOV) issues and makes training more efficient."
      ],
      "metadata": {
        "id": "rSHYizrHMijm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## GPT2 tokenizer is a pretrained model that used BPE (Byte Pair Encoding) for subword tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "encoded = tokenizer(text)"
      ],
      "metadata": {
        "id": "ZEwHkBB6NOcG"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow8K9qUdQlte",
        "outputId": "d9a9d338-2257-41a6-cbee-34d7125bf1be"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30642, 1634, 318, 262, 8489, 286, 399, 19930, 13, 632, 5419, 2270, 2420, 656, 4991, 13, 2141, 345, 4236, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(encoded.input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdXD2KOdNwsv",
        "outputId": "bfea2b20-6368-4e7b-917a-2c16af2a176b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Token', 'ization', 'Ġis', 'Ġthe', 'Ġfoundation', 'Ġof', 'ĠN', 'LP', '.', 'ĠIt', 'Ġhelps', 'Ġbreak', 'Ġtext', 'Ġinto', 'Ġunits', '.', 'ĠDo', 'Ġyou', 'Ġagree', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Please look up these concepts for furthur understanding\n",
        "\n",
        "1.   Why is there a Ġ before every decoded token in section 4?\n",
        "2.   what is OOV?\n",
        "3.   what is BPE?\n",
        "\n"
      ],
      "metadata": {
        "id": "pGfUiSQbPZWs"
      }
    }
  ]
}